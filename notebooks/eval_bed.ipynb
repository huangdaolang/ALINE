{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate BED\n",
    "\n",
    "Evaluate aline on BED tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "import random\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "\n",
    "from utils import set_seed, load_state_dict\n",
    "from utils.eval import eval_boed\n",
    "\n",
    "from utils import create_target_mask, select_targets_by_mask, compute_ll\n",
    "\n",
    "from model import BaseTransformer\n",
    "\n",
    "COLORS = ['#0072B2', '#009E73', '#D55E00', '#CC79A7', '#F0E442', '#56B4E9']\n",
    "plt.style.use(['seaborn-v0_8-colorblind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(config_overrides=[], state_dict=None, config_name=\"train_bed\", verbose=False):\n",
    "    # Initialise the hyper params\n",
    "    with initialize(version_base=None, config_path=\"./config\"):\n",
    "        cfg = compose(config_name=config_name, overrides=config_overrides)\n",
    "        \n",
    "    if verbose:\n",
    "        print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "\n",
    "    # Setting device\n",
    "    if not torch.cuda.is_available():\n",
    "        cfg.device = \"cpu\"\n",
    "    torch.set_default_device(cfg.device)\n",
    "    if cfg.device == \"cuda\":\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        torch.set_default_device(\"cuda\")\n",
    "\n",
    "    # Setting random seed\n",
    "    if cfg.fix_seed:\n",
    "        set_seed(cfg.seed)\n",
    "    else:\n",
    "        cfg.seed = torch.random.seed()\n",
    "\n",
    "    # Data\n",
    "    experiment = instantiate(cfg.task)\n",
    "\n",
    "    # Model\n",
    "    embedder = instantiate(cfg.embedder)\n",
    "    encoder = instantiate(cfg.encoder)\n",
    "    head = instantiate(cfg.head)\n",
    "    model = BaseTransformer(embedder, encoder, head)\n",
    "    \n",
    "\n",
    "    if state_dict is not None:\n",
    "        model = load_state_dict(model, cfg.output_dir, state_dict)\n",
    "\n",
    "\n",
    "    return cfg, experiment, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EIG Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_query = 2000\n",
    "\n",
    "cfg, experiment, model = load([f\"task.n_query_init={n_query}\"], \"aline_loc.pth\")\n",
    "\n",
    "bounds = eval_boed(model, experiment, cfg.T-1, int(1e3), 2000, 40, cfg.time_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_policy2d(model, experiment, T=30, N=200, title=\"\", posterior=True):\n",
    "    # Get trace\n",
    "    model.eval()\n",
    "    design_indices = []         # action: indices of the chosen designs\n",
    "    action_log_probs = []       # log probs of design history\n",
    "    full_action_probs = []  # log probs of all designs\n",
    "    nlls_for_prediction = []\n",
    "    nlls_for_query = []\n",
    "\n",
    "    experiment.n_query_init = N\n",
    "\n",
    "    batch = experiment.sample_batch(1)\n",
    "\n",
    "    # T-steps experiment\n",
    "\n",
    "    mask_type = random.choice(cfg.task.mask_type)\n",
    "    batch.target_mask = create_target_mask(mask_type,\n",
    "                                        cfg.task.embedding_type,\n",
    "                                        cfg.task.n_target_data,\n",
    "                                        cfg.task.n_target_theta,\n",
    "                                        cfg.task.n_selected_targets,\n",
    "                                        cfg.task.predefined_masks,\n",
    "                                        cfg.task.predefined_mask_weights,\n",
    "                                        cfg.task.mask_index,\n",
    "                                        cfg.task.attend_to)\n",
    "\n",
    "    for t in range(T):\n",
    "        pred = model.forward(batch)\n",
    "\n",
    "        idx = pred.design_out.idx                           # [B, 1]\n",
    "        design_indices.append(idx)\n",
    "\n",
    "        # Update the batch\n",
    "        batch = experiment.update_batch(batch, idx)\n",
    "\n",
    "        # Action log probs\n",
    "        action_log_probs.append(pred.design_out.log_prob)   # [B]\n",
    "        full_action_probs.append(pred.design_out.zt)    # [B, N_design]\n",
    "\n",
    "        # NLLs\n",
    "        target_ll = compute_ll(batch.target_all,\n",
    "                                pred.posterior_out.mixture_means,\n",
    "                                pred.posterior_out.mixture_stds,\n",
    "                                pred.posterior_out.mixture_weights)  # [B, n_target]\n",
    "\n",
    "        masked_target_ll = select_targets_by_mask(target_ll, batch.target_mask)\n",
    "\n",
    "        if cfg.task.embedding_type == \"mix\" and mask_type == \"all\":\n",
    "            nll_for_query = - (masked_target_ll[:, :-cfg.task.n_target_theta].mean(dim=-1) +\n",
    "                                masked_target_ll[:, -cfg.task.n_target_theta:].mean(dim=-1))\n",
    "        else:\n",
    "            nll_for_query = - masked_target_ll.mean(dim=-1)\n",
    "        nlls_for_query.append(nll_for_query)\n",
    "\n",
    "        if cfg.task.embedding_type == \"mix\":\n",
    "            nll = - (target_ll[:, :-cfg.task.n_target_theta].mean(dim=-1) +\n",
    "                        target_ll[:, -cfg.task.n_target_theta:].mean(dim=-1))\n",
    "        else:\n",
    "            nll = - target_ll.mean(dim=-1)\n",
    "        nlls_for_prediction.append(nll)\n",
    "\n",
    "    log_probs = torch.stack(action_log_probs, dim=1)\n",
    "\n",
    "\n",
    "    norm = plt.Normalize(0, 1)  # Normalize colors between 0 and 1\n",
    "    log_probs_exp = log_probs[0].exp().cpu().numpy()  # Convert log probs to probabilities\n",
    "    alpha_values = log_probs_exp # / log_probs_exp.max()  # Normalize alpha between 0 and 1\n",
    "    color_values = np.arange(1, T+1, 1) / T  # Normalize time steps to [0,1]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Plot the posterior\n",
    "    if posterior:\n",
    "        # For contour plot\n",
    "        num_point = 200\n",
    "        x = torch.linspace(0, 1, num_point)\n",
    "        y = torch.linspace(0, 1, num_point)\n",
    "        X, Y = torch.meshgrid(x, y)\n",
    "        pos = torch.stack([X, Y], dim=-1).reshape(1, -1, 2, 1)\n",
    "\n",
    "        prob = compute_ll(pos, \n",
    "                          pred.posterior_out.mixture_means,\n",
    "                                pred.posterior_out.mixture_stds,\n",
    "                                pred.posterior_out.mixture_weights).sum(-1)[0]\n",
    "        \n",
    "        contourf = ax.contourf(X.cpu().numpy(), Y.cpu().numpy(), prob.reshape(num_point, num_point).cpu().numpy(), 16, cmap=mpl.cm.bone) # PuBu_r\n",
    "\n",
    "        # add color bar\n",
    "        cbar_pos = fig.colorbar(contourf, ax=ax)\n",
    "        cbar_pos.set_label(r'Posterior $\\log q(\\theta \\, | \\,  \\mathcal{D}_T)$', \n",
    "                           rotation=270, verticalalignment='baseline')\n",
    "\n",
    "    # scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        batch.context_x[0, :T, 0].cpu().numpy(), \n",
    "        batch.context_x[0, :T, 1].cpu().numpy(), \n",
    "        c=color_values, cmap='summer', norm=norm, label=r'$\\xi_t$'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Add first colorbar for time step (color)\n",
    "    cbar1 = fig.colorbar(scatter, ax=ax)\n",
    "    cbar1.set_label(r'Time step $t/T$', rotation=270, verticalalignment='baseline', fontsize=12)\n",
    "\n",
    "\n",
    "    # Plot theta points\n",
    "    theta = batch.target_theta.reshape(-1, experiment.K, experiment.dim_x)\n",
    "    ax.scatter(theta[0, :, 0].cpu().numpy(), theta[0, :, 1].cpu().numpy(), color=COLORS[3], label=r'$\\theta$', marker='*', s=120)\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy2d(model, experiment, 30, title=\"Location Finding\", N=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EIG Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg, experiment, model = load([\"task=ces\", f\"task.n_query_init={n_query}\"], \"aline_ces.pth\")\n",
    "\n",
    "bounds = eval_boed(model, experiment, cfg.T-1, int(1e7), 2000, 10, cfg.time_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_posterior(model, experiment, T=30, N=200, title=\"CES\"):\n",
    "    # Evaluate model and generate data\n",
    "    model.eval()\n",
    "    experiment.n_query_init = N\n",
    "    batch = experiment.sample_batch(1)\n",
    "    theta = batch.target_theta.squeeze(-1)\n",
    "\n",
    "    # Run T steps\n",
    "    for t in range(T):\n",
    "        pred = model.forward(batch)\n",
    "        idx = pred.design_out.idx\n",
    "        batch = experiment.update_batch(batch, idx)\n",
    "\n",
    "    # Extract target variables\n",
    "    rho = theta[..., 0]\n",
    "    alpha = theta[..., 1:4]\n",
    "    log_u = theta[..., 4]\n",
    "\n",
    "    # Setup subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    line_width = 2\n",
    "    alpha_grid = 0.5\n",
    "\n",
    "    # --- Plot rho ---\n",
    "    ax = axes[0]\n",
    "    x = torch.linspace(0, 1, 100).unsqueeze(-1)\n",
    "    prob = compute_ll(x,\n",
    "                      pred.posterior_out.mixture_means[:, 0],\n",
    "                      pred.posterior_out.mixture_stds[:, 0],\n",
    "                      pred.posterior_out.mixture_weights[:, 0]).exp()\n",
    "    ax.plot(x.cpu().numpy(), prob.cpu().numpy(), linewidth=line_width)\n",
    "    ax.axvline(rho.cpu().numpy(), color='#f55f51', linestyle='--')\n",
    "    ax.set_title(r\"$\\rho$\")\n",
    "    ax.set_xlabel(r\"$\\rho$\")\n",
    "    # ax.set_ylabel(r\"$p(\\rho)$\")\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%.1f'))\n",
    "    # Add minor ticks\n",
    "    ax.xaxis.set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    # ax.grid(True, linestyle='--', alpha=alpha_grid)\n",
    "\n",
    "    # --- Plot alpha ---\n",
    "    ax = axes[1]\n",
    "    x = torch.linspace(0, 1, 100).unsqueeze(-1)\n",
    "    for i, a in enumerate(alpha.squeeze(0)):\n",
    "        prob = compute_ll(x,\n",
    "                          pred.posterior_out.mixture_means[:, i + 1],\n",
    "                          pred.posterior_out.mixture_stds[:, i + 1],\n",
    "                          pred.posterior_out.mixture_weights[:, i + 1]).exp()\n",
    "        ax.plot(x.cpu().numpy(), prob.cpu().numpy(), linewidth=line_width)\n",
    "        ax.axvline(a.cpu().numpy(), color='#f55f51', linestyle='--')\n",
    "    ax.set_title(r\"$\\alpha$\")\n",
    "    ax.set_xlabel(r\"$\\alpha$\")\n",
    "    # ax.set_ylabel(r\"$p(\\alpha)$\")\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%.1f'))\n",
    "    # Add minor ticks\n",
    "    ax.xaxis.set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    # ax.grid(True, linestyle='--', alpha=alpha_grid)\n",
    "\n",
    "    # --- Plot log(u) ---\n",
    "    ax = axes[2]\n",
    "    x = torch.linspace(-6, 8, 100).unsqueeze(-1)\n",
    "    prob = compute_ll(x,\n",
    "                      pred.posterior_out.mixture_means[:, 4],\n",
    "                      pred.posterior_out.mixture_stds[:, 4],\n",
    "                      pred.posterior_out.mixture_weights[:, 4]).exp()\n",
    "    ax.plot(x.cpu().numpy(), prob.cpu().numpy(), linewidth=line_width)\n",
    "    ax.axvline(log_u.cpu().numpy(), color='#f55f51', linestyle='--')\n",
    "    ax.set_title(r\"$u$\")\n",
    "    ax.set_xlabel(r\"$\\log(u)$\")\n",
    "    # ax.set_ylabel(r\"$p(\\log(u))$\")\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%.1f'))\n",
    "    # Add minor ticks\n",
    "    ax.xaxis.set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    # ax.grid(True, linestyle='--', alpha=alpha_grid)\n",
    "\n",
    "    # Add ONE y-label for the whole figure\n",
    "    fig.text(0.05, 0.5, r\"$p(\\theta)$\", va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "    # Final layout tweaks\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    fig.tight_layout(rect=[0.05, 0, 1, 1])\n",
    "    plt.savefig('outputs/figures/bed_pos_ces.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(model, experiment, T=30, N=n_query, title=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
